\section{MPI Edge Component}
Because there is no possibility to overcome the time complexity $\Omicron (3)$ in 3D and $\Omicron (2)$ in 2D For
investigating some huge structure, parallelization is a imperative support in FDTD. Recently famous Tactics to
parallelization include Cuda, Pthreads API, OpenMP API, and MPI standard. For well integrating with numpy used in
yaFDTD, mpi4py, the brother package implementing MPI Standard, was pick out here. 

In the slant of MPI, computational domain of FDTD can be splited into many fragments on different memory block and
manipulated by different CPU. Because data fragments are independent with others, at edges of computational domain, it
needs to exchange data. APIs of MPI serve here.

Invoking APIs of MPI in main script is a quick and dirty solution in vary examples and tutorials. It's possible wrapping
MPI actions in a class as a Decorator in our architecture. 
\begin{code}
  
\end{code}

